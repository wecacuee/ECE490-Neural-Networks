{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a54ad1",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff92772",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71bb1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch as t\n",
    "    import torch.nn as tnn\n",
    "except ImportError:\n",
    "    print(\"Colab users: pytorch comes preinstalled. Select Change Ru\")\n",
    "    print(\"Local users: Please install pytorch for your hardware using instructions from here: https://pytorch.org/get-started/locally/\")\n",
    "    print(\"ACG users: Please follow instructions here: https://vikasdhiman.info/ECE490-Neural-Networks/posts/0000-00-06-acg-slurm-jupyter/\")\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wget(url, filename):\n",
    "    \"\"\"\n",
    "    Download files using requests package. \n",
    "    Better than wget command line because this is cross platform.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import requests\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.call(\"pip install --user requests\".split())\n",
    "        import requests\n",
    "    r = requests.get(url)\n",
    "    with open(filename, 'wb') as fd:\n",
    "        for chunk in r.iter_content():\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training features from MNIST dataset.\n",
    "wget(\"https://vikasdhiman.info/ECE490-Neural-Networks/notebooks/05-mlp/zero_one_train_features.npz\", \n",
    "     \"zero_one_train_features.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_features(ax, zero_features, one_features):\n",
    "    zf = ax.scatter(zero_features[:, 0], zero_features[:, 1], marker='.', label='0', alpha=0.5)\n",
    "    of = ax.scatter(one_features[:, 0], one_features[:, 1], marker='+', label='1', alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Feature 1: count of pixels')\n",
    "    ax.set_ylabel('Feature 2: Variance along x-axis')\n",
    "    return [zf, of] # return list of artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "zero_one_train_features = np.load('zero_one_train_features.npz')\n",
    "FEATURE_MEAN = zero_one_train_features['mean']\n",
    "FEATURE_STD = zero_one_train_features['std']\n",
    "features = zero_one_train_features['normed_features']\n",
    "labels = zero_one_train_features['labels']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "draw_features(ax, features[labels > 0, :], features[labels < 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f272422",
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    DEVICE=\"cuda\"\n",
    "elif t.mps.is_available():\n",
    "    DEVICE=\"mps\"\n",
    "else:\n",
    "    DEVICE=\"cpu\"\n",
    "    \n",
    "DTYPE = t.get_default_dtype()\n",
    "\n",
    "def loss(predicted_labels, true_labels):\n",
    "    # Make sure predicted_labels and true_labels have same shape\n",
    "    y = true_labels[..., None]\n",
    "    yhat = predicted_labels\n",
    "    assert y.shape == yhat.shape\n",
    "    return t.maximum(- y * yhat, t.Tensor([0.]).to(device=DEVICE)).sum() / y.shape[-1]\n",
    "\n",
    "# TODO:\n",
    "# Define model = ?\n",
    "model = tnn.Sequential(\n",
    "    tnn.Linear(2, 5),\n",
    "    tnn.ReLU(),\n",
    "    tnn.Linear(5, 1))\n",
    "\n",
    "\n",
    "def train_by_gradient_descent(model, loss, train_features, train_labels, lr=0.0001):\n",
    "    predicted_labels = model(train_features)\n",
    "    #print(predicted_labels)\n",
    "    \n",
    "    loss_t = loss(predicted_labels, train_labels)\n",
    "    loss_t.backward()\n",
    "    loss_t_minus_1 = 2*loss_t  # Fake  value to make the while test pass once\n",
    "    niter = 0\n",
    "    while t.abs(loss_t - loss_t_minus_1) / loss_t > 0.01: # Stopping criterion\n",
    "        with t.no_grad(): # parameter update needs no gradients\n",
    "            for param in model.parameters():\n",
    "                assert param.grad is not None\n",
    "                param.add_( - lr * param.grad)  # Gradient descent\n",
    "                \n",
    "        model.zero_grad()\n",
    "        # Recompute the gradients\n",
    "        predicted_labels = model(train_features)\n",
    "        loss_t_minus_1 = loss_t\n",
    "        loss_t = loss(predicted_labels, train_labels)\n",
    "        loss_t.backward() # Compute gradients for next iteration\n",
    "        \n",
    "        # If loss increased, decrease lr. Works for gradient descent, not for stochatic gradient descent.\n",
    "        if loss_t > loss_t_minus_1:\n",
    "            lr = lr / 2\n",
    "        \n",
    "        ### DEBUGing information\n",
    "        iswrong = (train_labels * predicted_labels.ravel()) < 0\n",
    "        misclassified = (iswrong).sum() / iswrong.shape[0]\n",
    "        print(f\"loss: {loss_t:04.04f}, delta loss: {loss_t - loss_t_minus_1:04.04f},\" \n",
    "              f\"train misclassified: {misclassified:04.04f}\")\n",
    "        if niter % 20 == 0: # plot every 20th iteration\n",
    "            train_features_cpu = train_features.cpu()\n",
    "            predicted_labels_cpu = predicted_labels.cpu()\n",
    "            fig, ax = plt.subplots(1,1)\n",
    "            draw_features(ax, \n",
    "                          train_features_cpu[predicted_labels_cpu.ravel() > 0, :], \n",
    "                          train_features_cpu[predicted_labels_cpu.ravel() < 0, :])\n",
    "        \n",
    "        \n",
    "        niter += 1\n",
    "    return model\n",
    "\n",
    "\n",
    "trained_model = train_by_gradient_descent(model.to(device=DEVICE), \n",
    "                                          loss, \n",
    "                                          t.from_numpy(features).to(device=DEVICE, dtype=DTYPE),\n",
    "                                          t.from_numpy(labels).to(device=DEVICE, dtype=DTYPE))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,2)\n",
    "draw_features(axes[0], features[labels > 0, :], features[labels < 0, :])\n",
    "axes[0].set_title('Train labels')\n",
    "\n",
    "predicted_labels = trained_model(t.from_numpy(features).to(device=DEVICE, dtype=DTYPE))\n",
    "predicted_labels_cpu = predicted_labels.cpu()\n",
    "draw_features(axes[1], features[predicted_labels_cpu.ravel() > 0, :], \n",
    "                  features[predicted_labels_cpu.ravel() < 0, :])\n",
    "axes[1].set_title('Predicted labels');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5fea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doing it the Pytorch way without using our custom feature extraction\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(17)\n",
    "\n",
    "# Getting the dataset, the Pytorch way\n",
    "all_training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = torch.utils.data.random_split(all_training_data, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2958611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "learning_rate = 1e-3 # controls how fast the \n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "momentum = 0.9\n",
    "\n",
    "training_dataloader = DataLoader(training_data, shuffle=True, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(validation_data,  batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data,  batch_size=batch_size)\n",
    "  \n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# TODO:\n",
    "# Define model = ?\n",
    "\n",
    "model = tnn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    tnn.Linear(28*28, 10),\n",
    "    tnn.ReLU(),\n",
    "    tnn.Linear(10, 10))\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "def loss_and_accuracy(model, loss, validation_dataloader, device=DEVICE):\n",
    "        # Validation loop\n",
    "        validation_size = len(validation_dataloader.dataset)\n",
    "        num_batches = len(validation_dataloader)\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in validation_dataloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(X)\n",
    "                test_loss += loss(pred, y).item()\n",
    "                correct += (pred.argmax(dim=-1) == y).type(DTYPE).sum().item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= validation_size\n",
    "        return test_loss, correct\n",
    "    \n",
    "def train(model, loss, training_dataloader, validation_dataloader, device=DEVICE):\n",
    "    model.to(device)\n",
    "    for t in range(epochs):\n",
    "        # Train loop\n",
    "        training_size = len(training_dataloader.dataset)\n",
    "        for batch, (X, y) in enumerate(training_dataloader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "            loss_t = loss(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss_t.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss_t, current = loss_t.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss_t:>7f}  [{current:>5d}/{training_size:>5d}]\", end=\"\\r\")\n",
    "        valid_loss, correct = loss_and_accuracy(model, loss, validation_dataloader, device=device)\n",
    "        print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {valid_loss:>8f} \\n\")\n",
    "    return model\n",
    "        \n",
    "trained_model = train(model, loss, training_dataloader, validation_dataloader)\n",
    "\n",
    "test_loss, correct = loss_and_accuracy(model, loss, test_dataloader)\n",
    "print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5535b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = next(iter(test_dataloader))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99748f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The predicted image label is \", model(X.to(DEVICE)).argmax(dim=-1)[0].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
