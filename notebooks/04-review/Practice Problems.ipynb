{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9c7bed-df06-4634-baef-30c1a0a8381f",
   "metadata": {},
   "source": [
    "# Practice Problems for Midterm 1\n",
    "\n",
    "1. Calculators are allowed\n",
    "2. Computers are not allowed\n",
    "3. Show your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d559973",
   "metadata": {},
   "source": [
    "## Python Basics\n",
    "\n",
    "The midterm will be on paper, no computers will be allowed. Make sure you know what the python code output should be.\n",
    "\n",
    "Python questions will be restriced to content covered in Python_1.ipynb and Python_2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8525873-b1ad-4be1-94d1-606095e85df0",
   "metadata": {},
   "source": [
    "##### Q1. What will the following code print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = \"'Hello'\"\n",
    "name = '\"ECE\"'\n",
    "pi = 3.1419\n",
    "print(f'{hello:s} {name}. pi is {pi:.03f}')  # string formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a4622-b2b2-419c-a603-d41cd3003119",
   "metadata": {},
   "source": [
    "##### Q2. What will the following code print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1905200-14e2-4f55-a04b-cd296dd4ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [1, 2, 3, 'hello', [4, 5, 6]]    # Create a list\n",
    "print(xs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e178701-325e-4e3a-ba20-a03403cd3134",
   "metadata": {},
   "source": [
    "##### Q3. What will the following code print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f2ecd-cf1c-4b69-948e-772b2d8511da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = list(range(5))    # range is a built-in function that creates a list of integers\n",
    "print(nums[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f98fa-ba06-4d9b-b7b8-93e676175036",
   "metadata": {},
   "source": [
    "##### Q4. Which code is faster? Option 1 or Option 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ef45b-3085-4a5d-82d9-05f1b517a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Option 1:\n",
    "d = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data\n",
    "print(d['dog'])\n",
    "# Code option 2:\n",
    "keys = ['cat', 'dog'] # Create the dictionary with keys as lists\n",
    "values = ['cute', 'furry'] # # Create the dictionary with values as lists\n",
    "print(values[keys.index('dog')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e2535-e051-4582-aef6-b6521259aa84",
   "metadata": {},
   "source": [
    "##### Q5. Which code is faster? Option 1 or Option 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d166522-961f-40f5-aed3-699fad4938bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Option 1:\n",
    "d = {0: 'cute', 1: 'furry'}  # Create a new dictionary with some data\n",
    "print(d[1])\n",
    "# Code option 2:\n",
    "values = ['cute', 'furry'] # # Create the dictionary with values as lists\n",
    "print(values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003dba8-090c-443c-8639-597a8c0f97b2",
   "metadata": {},
   "source": [
    "##### Q6. What is the output of the following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86765df1-5e0f-424d-98e6-9629892823f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, v):\n",
    "        self.v = v\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return self.v * other\n",
    "\n",
    "print(Value(3) + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ac162-6e58-43fe-86b0-7bee84f07fdd",
   "metadata": {},
   "source": [
    "## Numpy basics\n",
    "\n",
    "Python questions will be restriced to content covered in NumpyTutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5df094-3c63-4754-aaae-a0c01975145f",
   "metadata": {},
   "source": [
    "##### Q7: What is the output of the following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c0649-167a-4956-9720-0d3e0dc867e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([[5, 6]])\n",
    "np.concatenate((x.T, y.T), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb596d3-9422-4328-948a-f885eddcec8a",
   "metadata": {},
   "source": [
    "##### Q8. What is the output of the following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf1b89-bf3f-479c-b563-a0c6de7bee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([[5, 6]])\n",
    "x @ y.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331dfd8f-cdff-498c-b0f3-ba1bd2c9d154",
   "metadata": {},
   "source": [
    "##### Q9. What is the output of the following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290d466-04a4-4b4c-9ac9-f19ca047cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([[5, 6]])\n",
    "(x * y).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6fc16-3de4-4ac8-86e6-03a3088d8ba6",
   "metadata": {},
   "source": [
    "## Linear algebra and it's geometry\n",
    "\n",
    "$\\newcommand{\\bfa}{\\mathbf{a}}$\n",
    "##### Q10. \n",
    "Show that for any vector $\\bfa = [a_1, a_2, \\dots, a_n]$, it's magnitude squared is same as dot product with itself i.e. $\\|\\bfa\\|^2 = \\bfa^\\top \\bfa$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e621ade3-bd54-4772-b53c-87a23695c6bb",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-088f9689ec190305",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A10. The mangitude of n-D vector is given by $\\|\\bfa\\| = \\sqrt{a_1^2 + a_2^2 + \\dots + a_n^2}$ and dot product the vector with itself is given by\n",
    "$\\bfa^\\top \\bfa = a_1 a_1 + a_2 a_2 + \\dots + a_n a_n = a_1^2 + a_2^2 + \\dots + a_n^2$. Squaring the magnitude gives us $\\|\\bfa\\| = a_1^2 + a_2^2 + \\dots + a_n^2$, which is same as $\\bfa^\\top \\bfa$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e595223-7248-42d6-8b8b-dbd40803d2a6",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bfv}{\\mathbf{v}}$\n",
    "$\\newcommand{\\bfu}{\\mathbf{u}}$\n",
    "$\\newcommand{\\bfx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bbR}{\\mathbb{R}}$\n",
    "$\\newenvironment{bm}{\\begin{bmatrix}}{\\end{bmatrix}}$\n",
    "$\\DeclareMathOperator{\\proj}{proj}$\n",
    "\n",
    "Q11. For given vectors $\\bfv$ and $\\bfu$ find the projection of $\\bfv$ on $\\bfu$ $\\proj_\\bfu \\bfv$. Also find the equation of dotted line which is perpendicular to $\\bfu$ and passes through $\\bfv$. Convert the equation of line to the form $y = m x + c$.\n",
    "\n",
    "$\\bfv = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} $ and $\\bfu = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$.\n",
    "\n",
    "![](https://openstax.org/apps/archive/20221219.191545/resources/263b8d95f699470f4cf6d49170b85118906c5ede)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d432dd1-85eb-4208-8c29-14d088063e6e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8f15613c87e7124f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A11. \n",
    "1. $\\proj_\\bfu \\bfv = \\bfv^\\top \\frac{\\bfu}{\\|\\bfu\\|} = \\frac{12}{\\sqrt{13}}$\n",
    "2. The dotted line is the set of all points $\\bfx \\in \\bbR^2$ that satisfy $\\bfu^\\top \\bfx = \\bfu^\\top \\bfv$ \n",
    "3. Let $\\bfx = [x, y]$. Then the above equation of line can be written as $[3, 2] \\begin{bm} x \\\\ y \\end{bm} = 12$ or $3x + 2y = 12$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887eff0e-b3e5-4be6-a037-bc3f1d87fc0e",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bfe}{\\mathbf{e}}$\n",
    "$\\newcommand{\\bfm}{\\mathbf{m}}$\n",
    "$\\newcommand{\\bfX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bfy}{\\mathbf{y}}$\n",
    "##### Q12. \n",
    "Convert the following scalar equation into vector form. Your end result should contain $\\bfm = [m; c]$, $\\bfy = [y_1; y_2; \\dots; y_n]$ and $\\bfx = [x_1; x_2, \\dots, x_n]$. You can define other vectors and matrices as needed, included a vector of ones like $\\mathbb{1}_n$.\n",
    "\n",
    "$$ e(m, c, (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)) = (y_1 - (x_1 m + c))^2 + (y_2 - (x_2 m + c))^2 + \\dots + (y_n - (x_n m + c))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960b458-dc4c-4a35-81fd-167474e73618",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bd8426e98700aab6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A12. \n",
    "Recall that the magnitude of a vector $ \\|\\bfv\\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^n}$ has a similar form to the error function. This suggests that we can define an error vector with the signed error for each data point as it's elements\n",
    "\n",
    "$$ \\bfe = \\begin{bmatrix}y_1 - (mx_1 + c)\\\\ y_2 - (mx_2 + c)\\\\ \\vdots \\\\ y_n - (mx_n + c)\\end{bmatrix}$$\n",
    "\n",
    "The total error is same as minimizing the square of error vector magnitude which is further same as vector product with itself.\n",
    "\n",
    "$$  e(m, c, (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)) = \\|\\bfe\\|^2 = \\bfe^\\top \\bfe$$\n",
    "\n",
    "Let us define $\\bfx = [x_1; \\dots; x_n]$ to denote the vector of all x coordinates of the dataset and $\\bfy = [y_1; \\dots; y_n]$ to denote y coordinates. Then the error vector is:\n",
    "$$ \\bfe = \\bfy - (\\bfx m +  \\mathbf{1}_n c)$$ \n",
    "\n",
    "where $\\mathbf{1}_n$ is a n-D vector of all ones. Finally, we vectorize parameters of the line $\\bfm = [m; c]$. We will also need to horizontally concatenate $\\bfx$ and $\\mathbf{1}_n$. Let's call the result $\\bfX = [\\bfx, \\mathbf{1}_n] \\in \\bbR^{n \\times 2}$. Now, the error vector looks like this:\n",
    "\n",
    "$$ \\bfe = \\bfy - \\bfX \\bfm$$ \n",
    "\n",
    "Expanding the error magnitude:\n",
    "\n",
    "$$ \\|\\bfe\\|^2 = (\\bfy - \\bfX \\bfm)^\\top (\\bfy - \\bfX \\bfm)\n",
    "\\\\\n",
    "= \\bfy^\\top\\bfy + \\bfm^\\top \\bfX^\\top \\bfX \\bfm - 2\\bfy^\\top \\bfX \\bfm \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99590ad4",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bfz}{\\mathbf{z}}$\n",
    "##### Q13: \n",
    "Convert the following scalar equation into vector form. Your end result should contain $\\bfm = [a; b; c]$, $\\bfz = [z_1; z_2; \\dots; z_n]$, $\\bfy = [y_1; y_2; \\dots; y_n]$ and $\\bfx = [x_1; x_2, \\dots, x_n]$. You can define other vectors and matrices as needed, included a vector of all ones like $\\mathbb{1}_n$.\n",
    "\n",
    "$$ e(a, b, c, (x_1, y_1, z_1), (x_2, y_2, z_2), \\dots, (x_n, y_n, z_n)) = (z_1 - (x_1 a + y_1 b + c))^2 + (z_2 - (x_2 a + y_2 b + c))^2 + \\dots + (z_n - (x_n a + y_n b + c))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0dd55",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-92d6319a0e7f33dc",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A13: A variation of A12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89874c8-f201-44e4-a12b-5505c744c4bd",
   "metadata": {},
   "source": [
    "##### Q14\n",
    "$\\newcommand{\\bfq}{\\mathbf{q}}$\n",
    "Convert the following vector equation into even more vectorized form. \n",
    "\n",
    "$$ e(m_0, \\bfm, (\\bfx_1, y_1), (\\bfx_2, y_2), \\dots, (\\bfx_n, y_n)) = (y_1 - (\\bfx_1^\\top \\bfm + m_0))^2 + (y_2 - (\\bfx_2^\\top \\bfm + m_0))^2 + \\dots + (y_n - (\\bfx_n^\\top \\bfm + m_0))^2$$\n",
    "\n",
    "where $\\bfm = [m_1; m_2; \\dots; m_p] \\in \\bbR^p$ is a p-dimensional vector and $\\bfx_i = [x_{i1}; x_{i2}; \\dots; x_{ip}] \\in \\bbR^p$ are p-dimensional vectors for all $i = \\{1, 2, \\dots n\\}$\n",
    "\n",
    "Your end result should contain $\\bfq = [m_0, m_1, m_2, \\dots, m_p] \\in \\bbR^{p+1}$, $\\bfy = [y_1; y_2; \\dots; y_n]\\in \\bbR^n$ and \n",
    "\n",
    "$$\\bfX = \n",
    "\\begin{bm}x_{11} & x_{12} & \\dots & x_{1p}\\\\\n",
    "x_{21} & x_{22} & \\dots & x_{2p} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "x_{n1} & x_{n2} & \\dots & x_{np} \\end{bm} \n",
    "= \\begin{bm}\n",
    "\\bfx_1^\\top \\\\\n",
    "\\bfx_2^\\top \\\\\n",
    "\\vdots\\\\\n",
    "\\bfx_n^\\top \\end{bm}\n",
    "\\in \\bbR^{n \\times p}$$. \n",
    "\n",
    "You can define other vectors and matrices as needed, included a vector of all ones like $\\mathbb{1}_n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cbf5e-9b5c-4be9-8dc2-d98a83939c55",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-25d55139f7aa4c61",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A15. \n",
    "Recall that the magnitude of a vector $ \\|\\bfv\\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^n}$ has a similar form to the error function. This suggests that we can define an error vector with the signed error for each data point as it's elements\n",
    "\n",
    "$$ \\bfe = \\begin{bmatrix}y_1 - (\\bfx_1^\\top\\bfm + m_0)\\\\ y_2 - (\\bfx_2^\\top\\bfm + m_0)\\\\ \\vdots \\\\ y_n - (\\bfx_2^\\top\\bfm_2 + m_0)\\end{bmatrix}$$\n",
    "\n",
    "The total error is same as minimizing the square of error vector magnitude which is further same as vector product with itself.\n",
    "\n",
    "$$  e(m_0, \\bfm, (\\bfx_1, y_1), (\\bfx_2, y_2), \\dots, (\\bfx_n, y_n)) = \\|\\bfe\\|^2 = \\bfe^\\top \\bfe$$\n",
    "\n",
    "Let us define $\\bfX = [\\bfx_1^\\top; \\dots; \\bfx_n^\\top]$ to denote the vector of all x coordinates of the dataset and $\\bfy = [y_1; \\dots; y_n]$ to denote y coordinates. Then the error vector is:\n",
    "$$ \\bfe = \\bfy - (\\mathbf{1}_n m_0 + \\bfX \\bfm)$$ \n",
    "\n",
    "where $\\mathbf{1}_n$ is a n-D vector of all ones. Finally, we call parameters of the line $\\bfq = [m_0; \\bfm]$. We will also need to horizontally concatenate $\\bfX$ and $\\mathbf{1}_n$. Let's call the result $\\bar{\\bfX} = [\\mathbf{1}_n, \\bfX] \\in \\bbR^{n \\times (p+1)}$. Now, the error vector looks like this:\n",
    "\n",
    "$$ \\bfe = \\bfy - \\bar{\\bfX} \\bfq$$ \n",
    "\n",
    "Expanding the error magnitude:\n",
    "$\\newcommand{\\bbfX}{\\bar{\\bfX}}$\n",
    "$$ \\|\\bfe\\|^2 = (\\bfy - \\bbfX \\bfq)^\\top (\\bfy - \\bbfX \\bfq)\n",
    "\\\\\n",
    "= \\bfy^\\top\\bfy + \\bfm^\\top \\bbfX^\\top \\bbfX \\bfq - 2\\bfy^\\top \\bbfX \\bfq \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d402c",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bfW}{\\mathbf{W}}$\n",
    "$\\DeclareMathOperator{\\Diag}{Diag}$\n",
    "##### Q16: \n",
    "Convert the following scalar equation into vector form. Your end result should contain $\\bfm = [m; c]$, the matrix $\\bfW = \\Diag([w_1; w_2; \\dots; w_n])$, $\\bfy = [y_1; y_2; \\dots; y_n]$ and $\\bfx = [x_1; x_2, \\dots, x_n]$. You can define other vectors and matrices as needed, included a vector of all ones like $\\mathbb{1}_n$.\n",
    "\n",
    "\n",
    "$$ e(m, c, (x_1, y_1, w_1), (x_2, y_2, w_2), \\dots, (x_n, y_n, w_n)) = w_1^2(y_1 - (x_1 m + c))^2 + w_2^2(y_2 - (x_2 m + c))^2 + \\dots + w_n^2(y_n - (x_n m + c))^2$$\n",
    "\n",
    "The matrix $\\bfW$ is defined as $\\Diag([w_1; w_2; \\dots; w_n])$ which indicates that $\\bfW$ is diagonal matrix of $[w_1; w_2; \\dots; w_n]$.\n",
    "\n",
    "$$ \\bfW = \\Diag([w_1; w_2; \\dots; w_n]) = \n",
    "\\begin{bm}w_1 & 0 & \\dots & 0\\\\\n",
    "0 & w_2 & \\dots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & w_n\\end{bm}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19fcf21",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8915c405ab33ad39",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A16:\n",
    "\n",
    "Recall that the magnitude of a vector $ \\|\\bfv\\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^n}$ has a similar form to the error function. This suggests that we can define an error vector with the signed error for each data point as it's elements\n",
    "\n",
    "$$ \\bfe = \\begin{bmatrix}y_1 - (mx_1 + c)\\\\ y_2 - (mx_2 + c)\\\\ \\vdots \\\\ y_n - (mx_n + c)\\end{bmatrix}$$\n",
    "and  let $\\bfW = \\Diag([w_1; w_2; \\dots; w_n])$.\n",
    "\n",
    "Note that \n",
    "$$ \\bfW \\bfe = \\begin{bmatrix}w_1(y_1 - (mx_1 + c))\\\\ w_2(y_2 - (mx_2 + c))\\\\ \\vdots \\\\ w_3(y_n - (mx_n + c))\\end{bmatrix}$$\n",
    "\n",
    "The total error is same as the square of error vector magnitude \n",
    "\n",
    "$$ e(m, c, (x_1, y_1, w_1), (x_2, y_2, w_2), \\dots, (x_n, y_n, w_n)) = w_1^2(y_1 - (x_1 m + c))^2 + w_2^2(y_2 - (x_2 m + c))^2 + \\dots + w_n^2(y_n - (x_n m + c))^2 = \\|\\bfW \\bfe\\|^2$$\n",
    "\n",
    "The square of error vector magnitude is same as dot product with itself,\n",
    "$$ \\|\\bfW \\bfe\\|^2 = (\\bfW\\bfe)^\\top(\\bfW\\bfe) = \\bfe^\\top \\bfW^\\top \\bfW \\bfe$$\n",
    "\n",
    "Let us define $\\bfx = [x_1; \\dots; x_n]$ to denote the vector of all x coordinates of the dataset and $\\bfy = [y_1; \\dots; y_n]$ to denote y coordinates. Then the error vector is:\n",
    "$$ \\bfe = \\bfy - (\\bfx m +  \\mathbf{1}_n c)$$ \n",
    "\n",
    "where $\\mathbf{1}_n$ is a n-D vector of all ones. Finally, we vectorize parameters of the line $\\bfm = [m; c]$. We will also need to horizontally concatenate $\\bfx$ and $\\mathbf{1}_n$. Let's call the result $\\bfX = [\\bfx, \\mathbf{1}_n] \\in \\bbR^{n \\times 2}$. Now, the error vector looks like this:\n",
    "\n",
    "$$ \\bfe = \\bfy - \\bfX \\bfm$$ \n",
    "\n",
    "Expanding the error magnitude:\n",
    "\n",
    "$$ \\|\\bfW\\bfe\\|^2 = (\\bfy - \\bfX \\bfm)^\\top \\bfW^\\top\\bfW (\\bfy - \\bfX \\bfm)\n",
    "\\\\\n",
    "= \\bfy^\\top\\bfW^\\top\\bfW\\bfy + \\bfm^\\top \\bfX^\\top \\bfW^\\top\\bfW\\bfX \\bfm - 2\\bfy^\\top \\bfW^\\top\\bfW\\bfX \\bfm \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4812bd1c",
   "metadata": {},
   "source": [
    "##### Q17:\n",
    "\n",
    "Using vector derivatives find the minimum of the following vector quadratic function in $\\bfm$:\n",
    "\n",
    "$$\\arg~\\min_{\\bfm} e(\\bfm) = \\bfy^\\top\\bfW^\\top\\bfW\\bfy + \\bfm^\\top \\bfX^\\top \\bfW^\\top\\bfW\\bfX \\bfm - 2\\bfy^\\top \\bfW^\\top\\bfW\\bfX \\bfm $$\n",
    "\n",
    "The dimensions of the each of the variables are given $\\bfm \\in \\bbR^p$, $\\bfy \\in \\bbR^n$, $\\bfW \\in \\bbR^{n \\times n}$, $\\bfX \\in \\bbR^{n \\times p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e375921-9d87-4cae-afda-af0d2f73b5c0",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-de247e8e2e79e04e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A17:\n",
    "$\\newcommand{\\p}{\\partial}$\n",
    "$\\newcommand{\\bfb}{\\mathbf{b}}$\n",
    "\\begin{align}\n",
    "\\mathbf{0}^\\top &= \\frac{\\p }{\\p \\bfm} (\\bfy^\\top\\bfW^\\top\\bfW\\bfy + \\bfm^\\top \\bfX^\\top \\bfW^\\top\\bfW\\bfX \\bfm - 2\\bfy^\\top \\bfW^\\top\\bfW\\bfX \\bfm)\\\\\n",
    "      &= 2 {\\bfm^*}^\\top \\bfX^\\top \\bfW^\\top \\bfW \\bfX  - 2\\bfy^\\top \\bfW^\\top \\bfW \\bfX\n",
    "\\end{align}\n",
    "\n",
    "This gives us the solution\n",
    "$$ \\bfm^* = (\\bfX^\\top \\bfW^\\top \\bfW \\bfX)^{-1} \\bfX^\\top \\bfW^\\top \\bfW \\bfy $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3deaf3-5ea3-4312-a50a-56c2b1a64892",
   "metadata": {},
   "source": [
    "## Vector derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30144d4e-eb5a-41c8-91b3-487bc6e8e04a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Q18:\n",
    "\n",
    "$\\newcommand{\\bfb}{\\mathbf{b}}$\n",
    "Find the derivative of $f(\\bfx) = (\\bfx - \\bfa_1)^\\top A (\\bfx - \\bfa_2)$ with respecto to $\\bfx$.\n",
    "\n",
    "You can assume $A \\in \\bbR^{n\\times n}$ to be symmetric. The size of vectors are $\\bfx, \\bfa_1, \\bfa_2, \\bfa_3, \\bfb \\in \\bbR^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987e4f0-5f42-45f4-82f5-40e4cc47f1b5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-64ce362c7b69280b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A18\n",
    "\n",
    "$$ f(\\bfx) = (\\bfx - \\bfa_1)^\\top A (\\bfx - \\bfa_2)\\\\\n",
    "= \\bfx^\\top A \\bfx - (\\bfa_1 + \\bfa_2)^\\top A \\bfx + \\bfa_1^\\top\\bfa_2$$ \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial \\bfx} = 2\\bfx^\\top A - (\\bfa_1 + \\bfa_2)^\\top A\\\\\n",
    "= (2\\bfx - (\\bfa_1 + \\bfa_2))^\\top A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ad915-3ee6-466c-bea7-70830cb5efc1",
   "metadata": {},
   "source": [
    "##### Q19:\n",
    "$\\newcommand{\\bfb}{\\mathbf{b}}$\n",
    "Find the quadratic approximation of the following function near the point $\\bfx_0$:\n",
    "\n",
    "$$ f(\\bfx) = \\left((\\bfx - \\bfa_1)^\\top A (\\bfx - \\bfa_2)\\right) \\left((\\bfx - \\bfa_3)^\\top \\bfb\\right) $$\n",
    "\n",
    "You can assume $A \\in \\bbR^{n\\times n}$ to be symmetric. The size of vectors are $\\bfx, \\bfa_1, \\bfa_2, \\bfa_3, \\bfb \\in \\bbR^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbb523-797d-4fe2-8de8-713f118970f3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c753f4ff3cfcfe99",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A19:\n",
    "\n",
    "$$ [\\nabla_\\bfx f(\\bfx)]^\\top = \\left((\\bfx - \\bfa_1)^\\top A (\\bfx - \\bfa_2)\\right) \\bfb^\\top+ ((\\bfx - \\bfa_3)^\\top \\bfb)\\left((2\\bfx - (\\bfa_1 + \\bfa_2))^\\top A\\right)$$\n",
    "$$ \\nabla_\\bfx f(\\bfx) = \\left((\\bfx - \\bfa_1)^\\top A (\\bfx - \\bfa_2)\\right) \\bfb + ((\\bfx - \\bfa_3)^\\top \\bfb)\\left(A (2\\bfx - (\\bfa_1 + \\bfa_2))\\right) $$\n",
    "\n",
    "$$ \\nabla_\\bfx f(\\bfx_0) = \\left((\\bfx_0 - \\bfa_1)^\\top A (\\bfx_0 - \\bfa_2)\\right) \\bfb + ((\\bfx_0 - \\bfa_3)^\\top \\bfb)\\left(A (2\\bfx_0 - (\\bfa_1 + \\bfa_2))\\right) $$\n",
    "\n",
    "$$ H f(\\bfx) = \\nabla^2_\\bfx f(\\bfx) =  \\bfb (2\\bfx - (\\bfa_1 + \\bfa_2))^\\top A \n",
    "+ \\left(A (2\\bfx - (\\bfa_1 + \\bfa_2))\\right)\\bfb^\\top \n",
    "+ \\left((\\bfx - \\bfa_3)^\\top \\bfb\\right)(2A) $$\n",
    "\n",
    "$$ H f(\\bfx_0) = \\nabla^2_\\bfx f(\\bfx_0) =  \\bfb (2\\bfx_0 - (\\bfa_1 + \\bfa_2))^\\top A \n",
    "+ \\left(A (2\\bfx_0 - (\\bfa_1 + \\bfa_2))\\right)\\bfb^\\top \n",
    "+ \\left((\\bfx_0 - \\bfa_3)^\\top \\bfb\\right)(2A) $$\n",
    "\n",
    "The quadratic approximation by Taylor series is:\n",
    "$$ f(\\bfx) = f(\\bfx_0) + [\\nabla_\\bfx f(\\bfx_0)]^\\top (\\bfx - \\bfx_0) + (\\bfx - \\bfx_0)^\\top H f(\\bfx_0) (\\bfx - \\bfx_0) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c0173-7114-4d07-b6e3-ec31577428d7",
   "metadata": {},
   "source": [
    "##### Q20\n",
    "\n",
    "$\\newcommand{\\bfc}{\\mathbf{c}}$\n",
    "$\\newcommand{\\p}{\\partial}$\n",
    "Show that for $\\bfc, \\bfx \\in \\bbR^n$\n",
    "$\\newcommand{\\bfc}{\\mathbf{c}}$\n",
    "$\\newcommand{\\bfA}{\\mathbf{A}}$\n",
    "\\begin{align}\n",
    "\\frac{\\p }{ \\p \\bfx} \\bfc^\\top \\bfx = \\bfc^\\top\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080da5bb-e446-4211-a497-cd8d9dbe28d0",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f6e7cd963019cee2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A20:\n",
    "Let $\\bfc = [c_1, c_2, \\dots, c_n]$ and $\\bfx = [x_1, x_2, \\dots x_n]$\n",
    "\n",
    "Let $f(\\bfx) = \\bfc^\\top \\bfx = c_1 x_1 + c_2 x_2 + \\dots c_n x_n$\n",
    "$\\newcommand{\\p}{\\partial}$\n",
    "\n",
    "$$\\frac{\\p f}{\\p x_1} = c_1\\\\\n",
    "\\frac{\\p f}{\\p x_2} = c_2\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p f}{\\p x_n} = c_n\\\\\n",
    "$$\n",
    "By Jacobian convention, we arrange the partial derivatives in a row vector:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\p }{ \\p \\bfx} \\bfc^\\top \\bfx = \n",
    "\\begin{bm} \\frac{\\p f}{\\p x_1} & \\frac{\\p f}{\\p x_2} & \\dots & \\frac{\\p f}{\\p x_n}\\end{bm}\n",
    "\\\\\n",
    "= \\begin{bm} c_1 & c_2 & \\dots & c_n\\end{bm} = \\bfc^\\top \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d0816-731b-4bca-a070-2f734cb2dcf4",
   "metadata": {},
   "source": [
    "##### Q21:\n",
    "\n",
    "Show that for $\\bfA \\in \\bbR^{n \\times n}$, $\\bfx \\in \\bbR^n$\n",
    "\\begin{align}\n",
    "\\frac{\\p }{ \\p \\bfx} \\bfA \\bfx = \\bfA\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff75c1e-b2b7-4dcc-8f84-73c44b9f9404",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2f66cd2ce7204676",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A21:\n",
    "Let $\\bfx = [x_1; x_2; \\dots x_n]$\n",
    "\n",
    "Let $\\bfA = \\begin{bm} a_{11} & a_{12}  & \\dots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{n1} & a_{n2} & \\dots & a_{nn} \\end{bm} \n",
    "= \\begin{bm} \\bfa_1^\\top \\\\ \\bfa_2^\\top \\\\ \\vdots \\\\ \\bfa_n^\\top \\end{bm}$,\n",
    "where $\\bfa_i^\\top  \\in \\bbR^{1 \\times n}$ are the row vectors of  matrix $\\bfA$.\n",
    "\n",
    "$\\newcommand{\\bff}{\\mathbf{f}}$\n",
    "Then $$ \\bfA \\bfx = \\begin{bm} \\bfa_1^\\top \\\\ \\bfa_2^\\top \\\\ \\vdots \\\\ \\bfa_n^\\top \\end{bm}\\bfx \n",
    "= \\begin{bm} \\bfa_1^\\top \\bfx \\\\ \\bfa_2^\\top \\bfx \\\\ \\vdots \\\\ \\bfa_n^\\top\\bfx  \\end{bm} $$\n",
    "\n",
    "Let \n",
    "$$\\bff(\\bfx) = \n",
    "\\begin{bm} f_1(\\bfx) \\\\ f_2(\\bfx) \\\\ \\vdots\\\\ f_n(\\bfx) \\end{bm}\n",
    "= \\bfA \\bfx = \\begin{bm} \\bfa_1^\\top \\bfx \\\\ \\bfa_2^\\top \\bfx \\\\ \\vdots \\\\ \\bfa_n^\\top\\bfx  \\end{bm}$$\n",
    "\n",
    "By Jacobian convention we arrange the partial derivatives of each function component column-wise\n",
    "\n",
    "$$ \\frac{\\p \\bff(\\bfx)}{\\p \\bfx} = \n",
    "\\begin{bm} \\frac{\\p f_1(\\bfx)}{\\p \\bfx} \\\\\n",
    "\\frac{\\p f_2(\\bfx)}{\\p \\bfx} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\p f_n(\\bfx)}{\\p \\bfx} \\end{bm} \n",
    "= \\begin{bm} \\frac{\\p \\bfa_1^\\top \\bfx}{\\p \\bfx} \\\\\n",
    "\\frac{\\p \\bfa_2^\\top \\bfx}{\\p \\bfx} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\p \\bfa_2^\\top \\bfx}{\\p \\bfx} \\end{bm} \n",
    "= \\begin{bm} \\bfa_1^\\top \\\\ \\bfa_2^\\top \\\\ \\vdots \\\\ \\bfa_n^\\top \\end{bm}\n",
    "= \\bfA\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae15c5c9-3982-46a4-bd98-1cdeb623d8e8",
   "metadata": {},
   "source": [
    "##### Q22:\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$\n",
    "Use vector-derivative chain rule:\n",
    "\n",
    "$$ \\frac{\\p \\bff(\\bfg(\\bfx)) }{\\p \\bfx} = \\frac{\\p \\bff}{\\p \\bfg}\\frac{\\p \\bfg}{\\p \\bfx}$$,\n",
    "\n",
    "for any function $\\bfg : \\bbR^n \\mapsto \\bbR^m$ and $\\bff : \\bbR^m \\mapsto \\bbR^o$.\n",
    "\n",
    "Show that for $\\bfx \\in \\bbR^n$ amd $\\bfA \\in \\bbR^{n \\times n}$\n",
    "$\\newcommand{\\bfA}{\\mathbf{A}}$\n",
    "\\begin{align}\n",
    "\\frac{\\p }{ \\p \\bfx} \\bfx^\\top \\bfA \\bfx = \\bfx^\\top (\\bfA^\\top + \\bfA)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1f07c-3891-4f30-909a-69f0e5c3f371",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-62982293cdd529df",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A22:\n",
    "\n",
    "For product of any two vectors\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfx} \\bfx^\\top \\bfy = \\bfy^\\top\n",
    "\\end{align}\n",
    "If $\\bfy$ is a function of $\\bfx$, then\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfx} \\bfx^\\top \\bfy &= \\bfy^\\top + \n",
    "\\left(\\frac{\\p }{\\p \\bfy} \\bfx^\\top \\bfy \\right) \\left(\\frac{\\p \\bfy }{\\p \\bfx}  \\right)\\\\\n",
    "&= \\bfy^\\top + \\bfx^\\top \\left(\\frac{\\p \\bfy }{\\p \\bfx}\\right)\n",
    "\\end{align}\n",
    "If $\\bfy = \\bfA \\bfx$, then \n",
    "$$\\frac{\\p \\bfy}{\\p \\bfx} = \\frac{\\p }{\\p \\bfx} \\bfA \\bfx = \\bfA$$\n",
    "and \n",
    "\n",
    "$$\\frac{\\p }{\\p \\bfx} \\bfx^\\top \\bfA \\bfx = \n",
    "\\bfy^\\top + \\bfx^\\top \\left(\\frac{\\p \\bfy }{\\p \\bfx}\\right)\n",
    "= \\bfx^\\top \\bfA^\\top + \\bfx^\\top \\bfA \n",
    "= \\bfx^\\top (\\bfA^\\top + \\bfA)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6455d414-9dbf-4328-95c9-9a5bb1b7424c",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff58c0a-8fbf-4074-a2ad-3075fbd5efb1",
   "metadata": {},
   "source": [
    "##### Q23:\n",
    "\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "You are given 2D points and corresponding labels as a training dataset $\\{ (x_1, y_1, l_1), (x_2, y_2, l_2), \\dots, (x_n, y_n, l_n) \\}$, where $x_i \\in \\bbR$, $y_i \\in \\bbR$ and the labels $l_i \\in \\{-1, 1\\}$. Use the model \n",
    "$\\hat{l}_i = \\sign(y_i - (m x_i + c))$ to construct a loss (or error) function. Find the gradient of the loss function with respect to the vector $\\bfm = [m; c]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22e867-2ce2-4322-816c-6b653b9e33c4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5196ee37af964cb0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A23\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$$ e(y_i, x_i; m,c) = \\begin{cases}\n",
    "0 &\\text{ if }\\sign(y_i - m x_i + c)  = l_i\\\\\n",
    "|y_i - (m x_i + c)| &\\text{ if }  \\sign(y_i - m x_i + c)  \\ne l_i\n",
    "\\end{cases}$$\n",
    "$\\newcommand{\\bfm}{\\mathbf{m}}$\n",
    "$$ e(y_i, x_i; m, c) = \\begin{cases}\n",
    "0 &\\text{ if } \\sign(y_i - m x_i + c)  = l_i\\\\\n",
    "|y_i - (m x_i + c)| &\\text{ if }  \\sign(y_i - m x_i + c)  \\ne l_i\n",
    "\\end{cases}$$\n",
    "\n",
    "$$\\bfm = \\begin{bmatrix}m \\\\ c\\end{bmatrix}$$\n",
    "\n",
    "$$ e(y_i, x_i;\\bfm) = \\begin{cases}\n",
    "0 &\\text{ if } \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm  = l_i\\\\\n",
    "|y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm| &\\text{ if }  \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm  \\ne l_i\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "If $l_i \\in \\{-1, 1\\}$, then we can write\n",
    "\n",
    "$$ e(y_i, x_i;\\bfm) =  \\max\\{0, - l_i (y_i -  \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm)\\} $$\n",
    "$$ \\nabla_\\bfm e(y_i, x_i;\\bfm) = \\max\\{0, l_i(\\begin{bmatrix} x_i& 1\\end{bmatrix})\\} $$\n",
    "\n",
    "$\\newcommand{\\bfx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bfy}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bfe}{\\mathbf{e}}$\n",
    "$\\newcommand{\\bfl}{\\mathbf{l}}$\n",
    "$\\newcommand{\\bfone}{\\mathbb{1}}$\n",
    "For the entire dataset, we have $\\bfy = [y_1; \\dots; y_n]$ and $\\bfx = [x_1; \\dots; x_n]$, $\\bfl = [l_1; \\dots; l_n]$ the average error is:\n",
    "$$ e(\\bfx, \\bfy; \\bfm) = \\frac{1}{n}{\\bfone_n^\\top}\\max\\{0, - \\bfl \\odot (\\bfy - \\begin{bmatrix}\\bfx  &  \\bfone_n\\end{bmatrix}\\bfm )\\}$$\n",
    "\n",
    "and the average gradient is:\n",
    "$$ \\nabla_\\bfm^\\top e(\\bfx, \\bfy; \\bfm) = \\frac{1}{n}{\\bfone_n^\\top}\\max\\{0,  \\bfl \\odot ( \\begin{bmatrix}\\bfx  &  \\bfone_n\\end{bmatrix} )\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fd2d5-7118-4204-bf6f-2fcda8578e43",
   "metadata": {},
   "source": [
    "##### Q24\n",
    "\n",
    "You are given p-D points $\\bfx_i \\in \\bbR^p$ and corresponding labels as a training dataset $\\{ (\\bfx_1, l_1), (\\bfx_2, l_2), \\dots, (\\bfx_n, l_n) \\}$, where $\\bfx_i \\in \\bbR^p$, and the labels $l_i \\in \\{-1, 1\\}$. Use the model \n",
    "$\\hat{l}_i = \\sign(\\bfx_i^\\top \\bfm + m_0))$ to construct a loss (or error) function. Find the gradient of the loss function with respect to the vector $\\bfq = [m_0; \\bfm]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7b84c-a84b-49c6-b805-e356256092c0",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d58b824e9a5ae937",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A24:\n",
    "\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$$ e(m_0, \\bfm; \\bfx_i) = \\begin{cases}\n",
    "0 &\\text{ if }\\sign(\\bfx_i^\\top \\bfm + m_0)  = l_i\\\\\n",
    "|\\bfx_i^\\top \\bfm + m_0| &\\text{ if }  \\sign(\\bfx_i^\\top \\bfm + m_0)  \\ne l_i\n",
    "\\end{cases}$$\n",
    "$\\newcommand{\\bfm}{\\mathbf{m}}$\n",
    "$$ e(y_i, x_i; m, c) = \\begin{cases}\n",
    "0 &\\text{ if } \\sign(\\bfx_i^\\top \\bfm + m_0)  = l_i\\\\\n",
    "|\\bfx_i^\\top \\bfm + m_0| &\\text{ if }  \\sign(\\bfx_i^\\top \\bfm + m_0)  \\ne l_i\n",
    "\\end{cases}$$\n",
    "\n",
    "$$\\bfq = \\begin{bmatrix}m_0 \\\\ \\bfm\\end{bmatrix}$$\n",
    "\n",
    "$$ e(m_0, \\bfm;\\bfx_i) = \\begin{cases}\n",
    "0 &\\text{ if } \\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}\\bfq  = l_i\\\\\n",
    "|\\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}\\bfq| &\\text{ if }  \\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}\\bfq  \\ne l_i\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "If $l_i \\in \\{-1, 1\\}$, then we can write\n",
    "\n",
    "$$ e(m_0, \\bfm;\\bfx_i) =  \\max\\{0, - l_i (\\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}\\bfq)\\} $$\n",
    "$$ \\nabla_\\bfm e(m_0, \\bfm;\\bfx_i) = \\max\\{0, - l_i (\\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix})\\} $$\n",
    "\n",
    "$\\newcommand{\\bfx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bfy}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bfe}{\\mathbf{e}}$\n",
    "$\\newcommand{\\bfl}{\\mathbf{l}}$\n",
    "$\\newcommand{\\bfone}{\\mathbb{1}}$\n",
    "For the entire dataset, we have $\\bfX = [\\bfx_1^\\top; \\dots; \\bfx_n^\\top]$, $\\bfl = [l_1; \\dots; l_n]$ the average error is:\n",
    "$$ e(\\bfm; \\bfX, \\bfl) = \\frac{1}{n}{\\bfone_n^\\top}\\max\\{0, - \\bfl \\odot ( \\begin{bmatrix}\\bfone_n & \\bfX \\end{bmatrix}\\bfq )\\}$$\n",
    "\n",
    "and the average gradient is:\n",
    "$$ \\nabla_\\bfm^\\top e(\\bfm; \\bfX, \\bfl) = \\frac{1}{n}{\\bfone_n^\\top}\\max\\{0,  \\bfl \\odot ( \\begin{bmatrix}  \\bfone_n & \\bfX \\end{bmatrix} )\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcaff62-98ea-4062-b823-494565076393",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a75362-cf0e-4865-aa4d-3e690fda8f24",
   "metadata": {},
   "source": [
    "##### Q25:\n",
    "\n",
    "Describe the Forward mode and reverse mode differentitation and their differences?\n",
    "\n",
    "Consider the following functions which one of the two will you use for:\n",
    "1. $\\bff(\\bfx) : \\bbR^2 \\mapsto \\bbR^{100}$\n",
    "2. $\\bff(\\bfx) : \\bbR^{100} \\mapsto \\bbR^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441eda75-7452-4fba-b038-af4ac89c24ea",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-efc5951441af47ae",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A25:\n",
    "\n",
    "$\\newcommand{\\bfh}{\\mathbf{h}}$\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$\n",
    "1. Forward mode and reverse mode differentiation differ by the order in which the chain rule jacobians get multiplied. For example, if you are required to take the derivative of the the function by chain rule $ \\bff(\\bfg(\\bfh(\\bfx)))$, where $\\bfh : \\bbR^n \\mapsto \\bbR^m $, $\\bfg : \\bbR^m \\mapsto \\bbR^o$, and $\\bfh: \\bbR^o \\mapsto \\bbR^p$ then by chain rule:\n",
    "$$ \\frac{\\p \\bff}{\\p \\bfx} = \\frac{\\p \\bff}{\\p \\bfg} \\frac{\\p \\bfg}{\\p \\bfh}\\frac{\\p \\bfh}{\\p \\bfx}$$\n",
    "\n",
    "There are two options for multipliying the jacobians\n",
    "\n",
    "a. Forward mode \n",
    "    $$\\left(\\frac{\\p \\bff}{\\p \\bfg} \\left(\\frac{\\p \\bfg}{\\p \\bfh}\\frac{\\p \\bfh}{\\p \\bfx}\\right)\\right)$$\n",
    "b. Reverse mode\n",
    "    $$\\left(\\left(\\frac{\\p \\bff}{\\p \\bfg} \\frac{\\p \\bfg}{\\p \\bfh}\\right)\\frac{\\p \\bfh}{\\p \\bfx}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08efe369-993f-4d50-b649-836c17d2dc5e",
   "metadata": {},
   "source": [
    "##### Q26:\n",
    "\n",
    "How many operations (additions and multiplications) does it take to multiple two matrices of size $A \\in \\bbR^{m \\times n}$ and $B\\in \\bbR^{n \\times p}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1cd8bd-82e4-4ab6-a694-63fb73171058",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8254904adca54017",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A26:\n",
    "$mp(2n -1)$.\n",
    "\n",
    "There exist matrix algorithms that are faster than $O(n^3)$. They speed up matrix multiplication to $O(n^{2.7})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab7bd8-3a51-4a24-acb9-ab7c8f949767",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Q27:\n",
    "$\\newcommand{\\bfB}{\\mathbf{B}}$\n",
    "Write the reverse mode vector-Jacobian product(s) for the following operations:\n",
    "\n",
    "1. $f(x) = exp(x)$ where $x \\in \\bbR$\n",
    "2. $\\bff(\\alpha, \\bfv) = \\alpha \\bfv$ where $\\alpha \\in \\bbR$ and $\\bfv \\in \\bbR^n$\n",
    "3. $f(\\bfa, \\bfb) = \\bfa^\\top \\bfb$ where $\\bfa \\in \\bbR^n$ and $\\bfb \\in \\bbR^n$\n",
    "4. $\\bff(\\bfA, \\bfb) = \\bfA \\bfb$ where $\\bfA \\in \\bbR^{m \\times n}$ and $\\bfb \\in \\bbR^n$\n",
    "5. $F(\\bfA, \\bfB) = \\bfA \\bfB$ where $\\bfA \\in \\bbR^{m \\times n}$ and $\\bfB \\in \\bbR^{n \\times p}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd87ba-5dbc-4fe8-935d-54ed36053063",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-463ca28ca37e32be",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A27:\n",
    "\n",
    "$\\newcommand{\\bfI}{\\mathbf{I}}$\n",
    "Let the vector be $\\frac{\\p l}{\\p f}$\n",
    "1. $\\frac{\\p l}{\\p x} = \\frac{\\p l}{\\p f} exp(x)$\n",
    "\n",
    "2. Let the vector be $\\frac{\\p l}{\\p \\bff}$. Then,\n",
    "$\\frac{\\p l}{\\p \\alpha} = \\frac{\\p l}{\\p \\bff} \\bfv $ and\n",
    "$\\frac{\\p l}{\\p \\bfv} = \\frac{\\p l}{\\p \\bff} \\alpha \\bfI_{n \\times n} $\n",
    "\n",
    "3. Let the vector be $\\frac{\\p l}{\\p f}$. Then\n",
    "$\\frac{\\p l}{\\p \\bfa} = \\frac{\\p l}{\\p f}\\bfb^\\top$ and\n",
    "$\\frac{\\p l}{\\p \\bfb} = \\frac{\\p l}{\\p f}\\bfa^\\top$\n",
    "\n",
    "4. Let the vector be $\\frac{\\p l}{\\p \\bff}$. Then\n",
    "$\\frac{\\p l}{\\p \\bfb} = \\frac{\\p l}{\\p \\bff}\\bfA$ and\n",
    "$\\frac{\\p l}{\\p \\bfA} = \\bfb \\frac{\\p l}{\\p \\bff}$.\n",
    "\n",
    "5. Let the vector be $\\frac{\\p l}{\\p F} \\in \\bbR^{p \\times m}$. Then\n",
    "$\\frac{\\p l}{\\p \\bfA} = \\bfB\\frac{\\p l}{\\p F}$ and\n",
    "$\\frac{\\p l}{\\p \\bfB} =  \\frac{\\p l}{\\p F}\\bfA$ and\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d0048-a663-4f30-b988-a923d610feb4",
   "metadata": {},
   "source": [
    "##### Q28:\n",
    "\n",
    "Write the forward-mode Jacobian-vector product(s) for the following operations\n",
    "\n",
    "1. $f(x) = exp(x)$ where $x \\in \\bbR$\n",
    "2. $\\bff(\\alpha, \\bfv) = \\alpha \\bfv$ where $\\alpha \\in \\bbR$ and $\\bfv \\in \\bbR^n$\n",
    "3. $f(\\bfa, \\bfb) = \\bfa^\\top \\bfb$ where $\\bfa \\in \\bbR^n$ and $\\bfb \\in \\bbR^n$\n",
    "4. $\\bff(\\bfA, \\bfb) = \\bfA \\bfb$ where $\\bfA \\in \\bbR^{m \\times n}$ and $\\bfb \\in \\bbR^n$\n",
    "5. $F(\\bfA, \\bfB) = \\bfA \\bfB$ where $\\bfA \\in \\bbR^{m \\times n}$ and $\\bfB \\in \\bbR^{n \\times p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081fc21-ff7c-4639-840a-f2452e9eaaeb",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a78b2bfa40819d61",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A28:\n",
    "    \n",
    "Let the vector be $\\frac{\\p x}{\\p t}$\n",
    "1. $\\frac{\\p f}{\\p t} =  exp(x) \\frac{\\p x}{\\p t}$\n",
    "\n",
    "2. Let the vectors be $\\frac{\\p \\alpha}{\\p t}$ and $\\frac{\\p \\bfv}{\\p t}$. Then,\n",
    "$\\frac{\\p \\bff}{\\p t} = \\bfv\\frac{\\p \\alpha}{\\p t} + \\alpha \\bfI_{n \\times n} \\frac{\\p \\bfv}{\\p t}$\n",
    "\n",
    "3. Let the vectors be $\\frac{\\p \\bfa}{\\p t}$ and $\\frac{\\p \\bfb}{\\p t}$. Then\n",
    "$\\frac{\\p f}{\\p t} = \\bfb^\\top\\frac{\\p \\bfa}{\\p t} + \\bfa^\\top \\frac{\\p \\bfb}{\\p t}$\n",
    "\n",
    "4. Let the vectors be $\\frac{\\p \\bfA}{\\p t}$ and $\\frac{\\p \\bfb}{\\p t}$. Then\n",
    "$\\frac{\\p \\bff}{\\p t} = \\frac{\\p \\bfA}{\\p t}\\bfb + \\bfA\\frac{\\p \\bfb}{\\p t}$\n",
    "\n",
    "\n",
    "5. Let the vectors be $\\frac{\\p \\bfA}{\\p t}$ and $\\frac{\\p \\bfB}{\\p t}$. Then\n",
    "$\\frac{\\p F}{\\p t} = \\frac{\\p \\bfA}{\\p t}\\bfB + \\bfA\\frac{\\p \\bfB}{\\p t}$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
